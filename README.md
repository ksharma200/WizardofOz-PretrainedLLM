# WizardofOz-PretrainedLLM from Scratch

PyTorch LLM from scratch pretrained using OpenWebText data (50 GB). Finetuned model trained using GPT-2 pretrained model and HuggingFace transformers with dataset: Wizard of Oz text with 100 pairs of prompts and responses. 

Trained on CPU device with 12 GB RAM. Total model training time = 41.5 hours.

## Query Screenshots

![image](https://github.com/ksharma200/WizardofOz-PretrainedLLM/assets/156555405/fb72f2fd-245e-4c6a-8f92-b4ecbd9f2954)

![image](https://github.com/ksharma200/WizardofOz-PretrainedLLM/assets/156555405/e83412df-3cba-4f83-af30-85de9bfe9cb1)


Inspired from FreeCodeCamp's Intro to LLMs course
Youtube: https://youtu.be/UU1WVnMk4E8?si=w_bZfxivekarZExx

