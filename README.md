# WizardofOz-PretrainedLLM from Scratch

PyTorch LLM from scratch pretrained using OpenWebText data (50 GB). Finetuned model trained using GPT-2 and HuggingFace transformers Wizard of Oz text with 100 pairs of prompts and responses. 

Trained on CPU device with 12 GB RAM. Total model training time = 41.5 hours.

## Query Screenshots

![image](https://github.com/ksharma200/WizardofOz-PretrainedLLM/assets/156555405/4884394e-ce31-4ea8-ae78-2f1a9d03a145)

![image](https://github.com/ksharma200/WizardofOz-PretrainedLLM/assets/156555405/1cb58f17-075a-4f6b-8909-b8c26c935fe6)


Inspired from FreeCodeCamp's Intro to LLMs course
Youtube: https://youtu.be/UU1WVnMk4E8?si=w_bZfxivekarZExx

