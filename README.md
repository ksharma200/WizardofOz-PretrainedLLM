# WizardofOz-PretrainedLLM from Scratch

PyTorch LLM from scratch pretrained using OpenWebText data (50 GB). Finetuned model trained using GPT-2 and HuggingFace transformers Wizard of Oz text with 100 pairs of prompts and responses. 

Trained on CPU device with 12 GB RAM. Total model training time = 41.5 hours.

Inspired from FreeCodeCamp's Intro to LLMs course
Youtube: https://youtu.be/UU1WVnMk4E8?si=w_bZfxivekarZExx

